{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# (a)\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, naive_bayes, metrics\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "yelp = pd.read_table(\"/Desktop/cs5785/hw3/sentiment_labelled_sentences/yelp_labelled.txt\", header = None, sep = \"\\t+\", engine = 'python').values\n",
    "amazon_cells = pd.read_table(\"/Desktop/cs5785/hw3/sentiment_labelled_sentences/amazon_cells_labelled.txt\", header = None, sep = \"\\t+\", engine = 'python').values\n",
    "imdb = pd.read_table(\"/Desktop/cs5785/hw3/sentiment_labelled_sentences/imdb_labelled.txt\", header = None, sep = \"\\t+\", engine = 'python').values\n",
    "print(yelp.shape)\n",
    "print(amazon_cells.shape)\n",
    "print(imdb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1's in yelp:  500\n",
      "Number of 0's in yelp:  500\n",
      "Number of 1's in amazon cells:  500\n",
      "Number of 0's in amazon cells:  500\n",
      "Number of 1's in imdb:  500\n",
      "Number of 0's in imdb:  500\n"
     ]
    }
   ],
   "source": [
    "print \"Number of 1's in yelp: \", np.sum(yelp[:, 1] != 0)\n",
    "print \"Number of 0's in yelp: \", np.sum(yelp[:, 1] == 0)\n",
    "print \"Number of 1's in amazon cells: \", np.sum(amazon_cells[:, 1] != 0)\n",
    "print \"Number of 0's in amazon cells: \", np.sum(amazon_cells[:, 1] == 0)\n",
    "print \"Number of 1's in imdb: \", np.sum(imdb[:, 1] != 0)\n",
    "print \"Number of 0's in imdb: \", np.sum(imdb[:, 1] == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2)\n",
      "(200, 2)\n",
      "(800, 2)\n",
      "(200, 2)\n",
      "(800, 2)\n",
      "(200, 2)\n",
      "(2400, 2) (600, 2)\n"
     ]
    }
   ],
   "source": [
    "# (b)-(g)\n",
    "\n",
    "yelp_train = np.append(yelp[np.where(yelp[:, 1] == 0)][:400], yelp[np.where(yelp[:, 1] != 0)][:400], axis = 0)\n",
    "print yelp_train.shape\n",
    "yelp_test = np.append(yelp[np.where(yelp[:, 1] == 0)][400:], yelp[np.where(yelp[:, 1] != 0)][400:], axis = 0)\n",
    "print yelp_test.shape\n",
    "\n",
    "amazon_cells_train = np.append(amazon_cells[np.where(amazon_cells[:, 1] == 0)][:400], amazon_cells[np.where(amazon_cells[:, 1] != 0)][:400], axis = 0)\n",
    "print amazon_cells_train.shape\n",
    "amazon_cells_test = np.append(amazon_cells[np.where(amazon_cells[:, 1] == 0)][400:], amazon_cells[np.where(amazon_cells[:, 1] != 0)][400:], axis = 0)\n",
    "print amazon_cells_test.shape\n",
    "\n",
    "imdb_train = np.append(imdb[np.where(imdb[:, 1] == 0)][:400], imdb[np.where(imdb[:, 1] != 0)][:400], axis = 0)\n",
    "print imdb_train.shape\n",
    "imdb_test = np.append(imdb[np.where(imdb[:, 1] == 0)][400:], imdb[np.where(imdb[:, 1] != 0)][400:], axis = 0)\n",
    "print imdb_test.shape\n",
    "\n",
    "train = np.append(np.append(yelp_train, amazon_cells_train, axis = 0), imdb_train, axis = 0)\n",
    "test = np.append(np.append(yelp_test, amazon_cells_test, axis = 0), imdb_test, axis = 0)\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfWordsVectorizer(object):\n",
    "    def __init__(self):\n",
    "        self.func_lemmatize = WordNetLemmatizer().lemmatize\n",
    "\n",
    "        self.STRIP_CHARS = string.punctuation + ' '\n",
    "        self.PUNCTUATIONS = set(string.punctuation)\n",
    "\n",
    "#         self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        self.STOP_WORDS = set([])\n",
    "\n",
    "\n",
    "        self.APOSTROPHE_DICT = {\"don't\": \"donot\", \"doesn't\": \"donot\", \"didn't\": \"donot\", \n",
    "                                \"isn't\": \"isnot\", \"aren't\": \"isnot\", \n",
    "                                \"i'm\": \"i\", \"you're\": \"you\", \"they're\": \"they\", \"he's\": \"he\", \"she's\": \"she\", \n",
    "                                \"it's\": \"it\", \"i've\": \"i\", \"you've\": \"you\",\n",
    "                                \"ha\": \"has\"}\n",
    "        self._vocabulary = None\n",
    "        self._lookup_voc = None\n",
    "        \n",
    "    def _strip_and_lemmatize(self, word, chars):\n",
    "#         word = str(self.func_lemmatize(word.strip(chars)))\n",
    "        word = str(self.func_lemmatize(word))\n",
    "        \n",
    "        if word in self.APOSTROPHE_DICT:\n",
    "            word = self.APOSTROPHE_DICT[word]\n",
    "            \n",
    "#         return word\n",
    "        return word.translate(None, self.STRIP_CHARS)\n",
    "    \n",
    "    def _tokenize_and_clean(self, sentence):\n",
    "#         _ = [self._strip_and_lemmatize(s, self.STRIP_CHARS) \n",
    "#              for s in sentence.lower().split(' ') if (s not in self.STOP_WORDS and len(s) > 0)]\n",
    "        _ = [self._strip_and_lemmatize(s, self.STRIP_CHARS) \n",
    "             for s in re.split(r'[^a-z\\']', sentence.lower()) if (s not in self.STOP_WORDS and len(s) > 0)]\n",
    "    \n",
    "        return filter(lambda s: len(s) > 0 and \n",
    "                               s not in self.STOP_WORDS and \n",
    "                               s not in self.PUNCTUATIONS, _)\n",
    "    def tokenize(self, X):\n",
    "        return map(self._tokenize_and_clean, X)\n",
    "    \n",
    "    \n",
    "    def _l1(self, x):\n",
    "        _l = len(x.nonzero()[0])\n",
    "        if _l > 0:\n",
    "            return x / float(_l)\n",
    "        else:\n",
    "            return np.zeros((len(x),))\n",
    "        \n",
    "    def _l2(self, x):\n",
    "        _l = len(x.nonzero()[0])\n",
    "        if _l > 0:\n",
    "            return x / float(np.sqrt(np.dot(x, x.T)))\n",
    "        else:\n",
    "            return np.zeros((len(x),))\n",
    "\n",
    "    def l1_normalize(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._l1(x), 1, X)\n",
    "\n",
    "    def l2_normalize(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._l2(x), 1, X)\n",
    "    \n",
    "    \n",
    "    def _bag_of_words(self, words):\n",
    "        bag = np.zeros((self._voc_len,), dtype=np.int)\n",
    "        for word in words:\n",
    "            try:\n",
    "                bag[self._vocabulary[word]] += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return bag\n",
    "    \n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, normalize=None):\n",
    "        # 1. build vocabulary\n",
    "        tokens = self.tokenize(X)\n",
    "        \n",
    "        _ = set(np.concatenate(tokens))\n",
    "        \n",
    "        self._lookup_voc = {}\n",
    "        self._vocabulary = {}\n",
    "        for i, x in enumerate(_):\n",
    "            self._lookup_voc[i] = x\n",
    "            self._vocabulary[x] = i\n",
    "        \n",
    "        self._voc_len = len(_)\n",
    "        \n",
    "        # 2. generate X\n",
    "        _ = np.array(map(self._bag_of_words, tokens))\n",
    "        if normalize is None:\n",
    "            return _\n",
    "        elif normalize == 'l2':\n",
    "            return self.l2_normalize(_)\n",
    "        else:\n",
    "            return self.l1_normalize(_)\n",
    "            \n",
    "    def transform(self, X, normalize=None):\n",
    "        tokens = self.tokenize(X)\n",
    "#         print tokens\n",
    "        if self._vocabulary is not None:\n",
    "            _ = np.array(map(self._bag_of_words, tokens)) \n",
    "            \n",
    "            if normalize is None:\n",
    "                return _\n",
    "            elif normalize == 'l2':\n",
    "                return self.l2_normalize(_)\n",
    "            else:\n",
    "                return self.l1_normalize(_)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramVectorizer(BagOfWordsVectorizer):\n",
    "    def __init__(self, N):\n",
    "        super(NGramVectorizer, self).__init__()\n",
    "        self.N = N\n",
    "        \n",
    "    def _gen_ngram(self, tokens):\n",
    "        r = []\n",
    "        for i in range(0, len(tokens) - self.N + 1):\n",
    "            r.append(\" \".join(tokens[i:i+self.N]))\n",
    "        return r\n",
    "        \n",
    "    def tokenize(self, X):\n",
    "        _ = super(NGramVectorizer, self).tokenize(X)\n",
    "        return map(self._gen_ngram, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train[:, 1].astype(int)\n",
    "test_labels = test[:, 1].astype(int)\n",
    "print train_labels.shape\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Word Model with Logistic Regression\n",
      "(2400, 4179)\n",
      "(600, 4179)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "Classification accuracy:  0.785\n",
      "[[243  57]\n",
      " [ 72 228]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour; \n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words\n",
    "# Logistic regression\n",
    "print \"Bag-of-Word Model with Logistic Regression\"\n",
    "\n",
    "normalize = 'l2'\n",
    "\n",
    "bowv = BagOfWordsVectorizer()\n",
    "train_bag = bowv.fit_transform(train[:,0], normalize)\n",
    "test_bag = bowv.transform(test[:,0], normalize)\n",
    "print(train_bag.shape)\n",
    "print(test_bag.shape)\n",
    "print(train_bag[500])\n",
    "print(train_bag[1000])\n",
    "\n",
    "cls_lr = linear_model.LogisticRegression()\n",
    "cls_lr.fit(train_bag, train_labels)\n",
    "score_lr = cls_lr.fit(train_bag, train_labels).score(test_bag, test_labels)\n",
    "pred_lr = cls_lr.predict(test_bag)\n",
    "\n",
    "print \"\\nClassification accuracy: \", score_lr\n",
    "print metrics.confusion_matrix(test_labels, pred_lr)\n",
    "print '-----'\n",
    "\n",
    "coef = cls.coef_[0]\n",
    "idx = np.argsort(coef)\n",
    "print 'Positive words: '\n",
    "for i in idx[::-1][:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n",
    "print '\\n-----'\n",
    "print 'Negative words: '\n",
    "for i in idx[:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Word Model with Naive Bayes\n",
      "(2400, 4179)\n",
      "(600, 4179)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "Classification accuracy:  0.81\n",
      "[[256  44]\n",
      " [ 70 230]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour; \n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words\n",
    "# Naive Bayes\n",
    "print \"Bag-of-Word Model with Naive Bayes\"\n",
    "\n",
    "normalize = 'l2'\n",
    "bowv = BagOfWordsVectorizer()\n",
    "train_bag = bowv.fit_transform(train[:,0], normalize)\n",
    "test_bag = bowv.transform(test[:,0], normalize)\n",
    "print(train_bag.shape)\n",
    "print(test_bag.shape)\n",
    "print(train_bag[500])\n",
    "print(train_bag[1000])\n",
    "\n",
    "cls_nb = naive_bayes.BernoulliNB()\n",
    "cls_nb.fit(train_bag, train_labels)\n",
    "score_nb = cls_nb.fit(train_bag, train_labels).score(test_bag, test_labels)\n",
    "pred_nb = cls_nb.predict(test_bag)\n",
    "\n",
    "print \"\\nClassification accuracy: \", score_nb\n",
    "print metrics.confusion_matrix(test_labels, pred_nb)\n",
    "print '-----'\n",
    "\n",
    "coef = cls.coef_[0]\n",
    "idx = np.argsort(coef)\n",
    "print 'Positive words: '\n",
    "for i in idx[::-1][:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n",
    "print '\\n-----'\n",
    "print 'Negative words: '\n",
    "for i in idx[:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model with Logistic Regression\n",
      "(2400, 16417)\n",
      "(600, 16417)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "Classification accuracy:  0.716666666667\n",
      "[[233  67]\n",
      " [103 197]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and; \n"
     ]
    }
   ],
   "source": [
    "# N-gram\n",
    "# Logistic regression\n",
    "print \"N-Gram Model with Logistic Regression\"\n",
    "\n",
    "normalize = 'l2'\n",
    "\n",
    "bowv = NGramVectorizer(2)\n",
    "train_ngram = bowv.fit_transform(train[:,0], normalize)\n",
    "test_ngram = bowv.transform(test[:,0], normalize)\n",
    "print(train_ngram.shape)\n",
    "print(test_ngram.shape)\n",
    "print(train_ngram[500])\n",
    "print(train_ngram[1000])\n",
    "\n",
    "cls_lr = linear_model.LogisticRegression()\n",
    "cls_lr.fit(train_ngram, train_labels)\n",
    "score_lr = cls_lr.fit(train_ngram, train_labels).score(test_ngram, test_labels)\n",
    "pred_lr = cls_lr.predict(test_ngram)\n",
    "\n",
    "print \"\\nClassification accuracy: \", score_lr\n",
    "print metrics.confusion_matrix(test_labels, pred_lr)\n",
    "print '-----'\n",
    "\n",
    "coef = cls.coef_[0]\n",
    "idx = np.argsort(coef)\n",
    "print 'Positive words: '\n",
    "for i in idx[::-1][:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n",
    "print '\\n-----'\n",
    "print 'Negative words: '\n",
    "for i in idx[:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model with Naive Bayes\n",
      "(2400, 16417)\n",
      "(600, 16417)\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "\n",
      "Classification accuracy:  0.74\n",
      "[[202  98]\n",
      " [ 58 242]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and; \n"
     ]
    }
   ],
   "source": [
    "# N-gram\n",
    "# Naive Bayes\n",
    "print \"N-Gram Model with Naive Bayes\"\n",
    "\n",
    "normalize = 'l2'\n",
    "\n",
    "bowv = NGramVectorizer(2)\n",
    "train_ngram = bowv.fit_transform(train[:,0], normalize)\n",
    "test_ngram = bowv.transform(test[:,0], normalize)\n",
    "print(train_ngram.shape)\n",
    "print(test_ngram.shape)\n",
    "print(train_ngram[500])\n",
    "print(train_ngram[1000])\n",
    "\n",
    "cls_nb = naive_bayes.BernoulliNB()\n",
    "cls_nb.fit(train_ngram, train_labels)\n",
    "score_nb = cls_nb.fit(train_ngram, train_labels).score(test_ngram, test_labels)\n",
    "pred_nb = cls_nb.predict(test_ngram)\n",
    "\n",
    "print \"\\nClassification accuracy: \", score_nb\n",
    "print metrics.confusion_matrix(test_labels, pred_nb)\n",
    "print '-----'\n",
    "\n",
    "coef = cls.coef_[0]\n",
    "idx = np.argsort(coef)\n",
    "print 'Positive words: '\n",
    "for i in idx[::-1][:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n",
    "print '\\n-----'\n",
    "print 'Negative words: '\n",
    "for i in idx[:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (h)\n",
    "\n",
    "class PCA(object):\n",
    "    def fit_transform(self, X, q):\n",
    "        self.u = np.mean(X, axis=0)\n",
    "        self._X = X - self.u\n",
    "        self.V = LA.svd(self._X)[-1]\n",
    "        return self._X.dot(self.V[:q, :].T)\n",
    "    \n",
    "    def transform(self, X, q):\n",
    "        return (X - self.u).dot(self.V[:q, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Word Model with Logistic Regression after PCA\n",
      "\n",
      "-----\n",
      "Rank:  10\n",
      "\n",
      "Classification accuracy:  0.583333333333\n",
      "[[197 103]\n",
      " [147 153]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour;  \n",
      "-----\n",
      "Rank:  50\n",
      "\n",
      "Classification accuracy:  0.671666666667\n",
      "[[210  90]\n",
      " [107 193]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour;  \n",
      "-----\n",
      "Rank:  100\n",
      "\n",
      "Classification accuracy:  0.716666666667\n",
      "[[219  81]\n",
      " [ 89 211]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour; \n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Word Model with Logistic Regression after PCA\n",
    "\n",
    "print \"Bag-of-Word Model with Logistic Regression after PCA\"\n",
    "\n",
    "normalize = 'l2'\n",
    "    \n",
    "for q in [10, 50, 100]:\n",
    "    print '\\n-----\\nRank: ', q\n",
    "    \n",
    "    bowv = BagOfWordsVectorizer()\n",
    "    pca = PCA()\n",
    "    train_bag = pca.fit_transform(bowv.fit_transform(train[:,0], normalize), q)\n",
    "    test_bag = pca.transform(bowv.transform(test[:,0], normalize), q)\n",
    "\n",
    "    cls_lr = linear_model.LogisticRegression()\n",
    "    cls_lr.fit(train_bag, train_labels)\n",
    "    score_lr = cls_lr.fit(train_bag, train_labels).score(test_bag, test_labels)\n",
    "    pred_lr = cls_lr.predict(test_bag)\n",
    "\n",
    "    print \"\\nClassification accuracy: \", score_lr\n",
    "    print metrics.confusion_matrix(test_labels, pred_lr)\n",
    "    print '-----'\n",
    "\n",
    "    coef = cls.coef_[0]\n",
    "    idx = np.argsort(coef)\n",
    "    print 'Positive words: '\n",
    "    for i in idx[::-1][:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],\n",
    "    print '\\n-----'\n",
    "    print 'Negative words: '\n",
    "    for i in idx[:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Word Model with Naive Bayes after PCA\n",
      "\n",
      "-----\n",
      "Rank:  10\n",
      "\n",
      "Classification accuracy:  0.555\n",
      "[[170 130]\n",
      " [137 163]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour;  \n",
      "-----\n",
      "Rank:  50\n",
      "\n",
      "Classification accuracy:  0.601666666667\n",
      "[[181 119]\n",
      " [120 180]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour;  \n",
      "-----\n",
      "Rank:  100\n",
      "\n",
      "Classification accuracy:  0.646666666667\n",
      "[[203  97]\n",
      " [115 185]]\n",
      "-----\n",
      "Positive words: \n",
      "great;  good;  love;  excellent;  best;  nice;  delicious;  amazing;  loved;  fantastic;  awesome;  well;  funny;  and;  liked;  recommend;  definitely;  friendly;  price;  happy;  \n",
      "-----\n",
      "Negative words: \n",
      "not;  bad;  donot;  poor;  worst;  terrible;  awful;  no;  disappointing;  never;  horrible;  there;  minute;  disappointed;  waste;  then;  at;  disappointment;  slow;  hour; \n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Word Model with Naive Bayes after PCA\n",
    "\n",
    "print \"Bag-of-Word Model with Naive Bayes after PCA\"\n",
    "\n",
    "normalize = 'l2'\n",
    "    \n",
    "for q in [10, 50, 100]:\n",
    "    print '\\n-----\\nRank: ', q\n",
    "    \n",
    "    bowv = BagOfWordsVectorizer()\n",
    "    pca = PCA()\n",
    "    train_bag = pca.fit_transform(bowv.fit_transform(train[:,0], normalize), q)\n",
    "    test_bag = pca.transform(bowv.transform(test[:,0], normalize), q)\n",
    "\n",
    "    cls_nb = naive_bayes.BernoulliNB()\n",
    "    cls_nb.fit(train_bag, train_labels)\n",
    "    score_nb = cls_nb.fit(train_bag, train_labels).score(test_bag, test_labels)\n",
    "    pred_nb = cls_nb.predict(test_bag)\n",
    "\n",
    "    print \"\\nClassification accuracy: \", score_nb\n",
    "    print metrics.confusion_matrix(test_labels, pred_nb)\n",
    "    print '-----'\n",
    "\n",
    "    coef = cls.coef_[0]\n",
    "    idx = np.argsort(coef)\n",
    "    print 'Positive words: '\n",
    "    for i in idx[::-1][:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],\n",
    "    print '\\n-----'\n",
    "    print 'Negative words: '\n",
    "    for i in idx[:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model with Logistic Regression after PCA\n",
      "\n",
      "-----\n",
      "Rank:  10\n",
      "\n",
      "Classification accuracy:  0.543333333333\n",
      "[[241  59]\n",
      " [215  85]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and;  \n",
      "-----\n",
      "Rank:  50\n",
      "\n",
      "Classification accuracy:  0.598333333333\n",
      "[[215  85]\n",
      " [156 144]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and;  \n",
      "-----\n",
      "Rank:  100\n",
      "\n",
      "Classification accuracy:  0.628333333333\n",
      "[[212  88]\n",
      " [135 165]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and; \n"
     ]
    }
   ],
   "source": [
    "# N-Gram Model with Logistic Regression after PCA\n",
    "\n",
    "print \"N-Gram Model with Logistic Regression after PCA\"\n",
    "\n",
    "normalize = 'l2'\n",
    "    \n",
    "for q in [10, 50, 100]:\n",
    "    print '\\n-----\\nRank: ', q\n",
    "    \n",
    "    bowv = NGramVectorizer(2)\n",
    "    pca = PCA()\n",
    "    train_ngram = pca.fit_transform(bowv.fit_transform(train[:,0], normalize), q)\n",
    "    test_ngram = pca.transform(bowv.transform(test[:,0], normalize), q)\n",
    "\n",
    "    cls_lr = linear_model.LogisticRegression()\n",
    "    cls_lr.fit(train_ngram, train_labels)\n",
    "    score_lr = cls_lr.fit(train_ngram, train_labels).score(test_ngram, test_labels)\n",
    "    pred_lr = cls_lr.predict(test_ngram)\n",
    "\n",
    "    print \"\\nClassification accuracy: \", score_lr\n",
    "    print metrics.confusion_matrix(test_labels, pred_lr)\n",
    "    print '-----'\n",
    "\n",
    "    coef = cls.coef_[0]\n",
    "    idx = np.argsort(coef)\n",
    "    print 'Positive words: '\n",
    "    for i in idx[::-1][:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],\n",
    "    print '\\n-----'\n",
    "    print 'Negative words: '\n",
    "    for i in idx[:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram Model with Naive Bayes after PCA\n",
      "\n",
      "-----\n",
      "Rank:  10\n",
      "\n",
      "Classification accuracy:  0.558333333333\n",
      "[[182 118]\n",
      " [147 153]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and;  \n",
      "-----\n",
      "Rank:  50\n",
      "\n",
      "Classification accuracy:  0.598333333333\n",
      "[[179 121]\n",
      " [120 180]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and;  \n",
      "-----\n",
      "Rank:  100\n",
      "\n",
      "Classification accuracy:  0.596666666667\n",
      "[[167 133]\n",
      " [109 191]]\n",
      "-----\n",
      "Positive words: \n",
      "unbearable after;  meat and;  cinema imaginable;  back many;  i now;  coastal retreat;  how hollywood;  walked in;  a stunning;  problem wit;  yell when;  builder were;  about useless;  created a;  with hatred;  player in;  that ballet;  such a;  also came;  cinematography isnot;  \n",
      "-----\n",
      "Negative words: \n",
      "nyc bagel;  of ebay;  wa pink;  lunch and;  play but;  road house;  so mature;  the selection;  it can;  people like;  fx are;  our side;  the wrong;  realize that;  behind this;  jabra behing;  out the;  working in;  authentic or;  upstairs and; \n"
     ]
    }
   ],
   "source": [
    "# N-Gram Model with Naive Bayes after PCA\n",
    "\n",
    "print \"N-Gram Model with Naive Bayes after PCA\"\n",
    "\n",
    "normalize = 'l2'\n",
    "    \n",
    "for q in [10, 50, 100]:\n",
    "    print '\\n-----\\nRank: ', q\n",
    "    \n",
    "    bowv = NGramVectorizer(2)\n",
    "    pca = PCA()\n",
    "    train_ngram = pca.fit_transform(bowv.fit_transform(train[:,0], normalize), q)\n",
    "    test_ngram = pca.transform(bowv.transform(test[:,0], normalize), q)\n",
    "\n",
    "    cls_nb = naive_bayes.BernoulliNB()\n",
    "    cls_nb.fit(train_ngram, train_labels)\n",
    "    score_nb = cls_nb.fit(train_ngram, train_labels).score(test_ngram, test_labels)\n",
    "    pred_nb = cls_nb.predict(test_ngram)\n",
    "\n",
    "    print \"\\nClassification accuracy: \", score_nb\n",
    "    print metrics.confusion_matrix(test_labels, pred_nb)\n",
    "    print '-----'\n",
    "\n",
    "    coef = cls.coef_[0]\n",
    "    idx = np.argsort(coef)\n",
    "    print 'Positive words: '\n",
    "    for i in idx[::-1][:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],\n",
    "    print '\\n-----'\n",
    "    print 'Negative words: '\n",
    "    for i in idx[:20]:\n",
    "        print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
